<div class="aws-blog-content lb-row lb-row-max-large lb-snap lb-gutter-mid">
 <main class="lb-col lb-tiny-24 lb-mid-16" id="aws-page-content-main" role="main" tabindex="-1">
  <h2 class="lb-h5 blog-title">
   <a href="https://aws.amazon.com/blogs/database/">
    AWS Database Blog
   </a>
  </h2>
  <article class="blog-post" typeof="TechArticle" vocab="https://schema.org/">
   <meta content="en-US" property="inLanguage"/>
   <meta content="https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2023/05/30/DBBLOG-3003-featured-images.jpg" property="image"/>
   <h1 class="lb-h2 blog-post-title" property="name headline">
    Cost-effective bulk processing with Amazon DynamoDB
   </h1>
   <footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
    by
    <span property="author" typeof="Person">
     <span property="name">
      Jason Hunter
     </span>
    </span>
    | on
    <time datetime="2023-05-30T10:31:05-07:00" property="datePublished">
     30 MAY 2023
    </time>
    | in
    <span class="blog-post-categories">
     <a href="https://aws.amazon.com/blogs/database/category/database/amazon-dynamodb/" title="View all posts in Amazon DynamoDB">
      <span property="articleSection">
       Amazon DynamoDB
      </span>
     </a>
     ,
     <a href="https://aws.amazon.com/blogs/database/category/learning-levels/expert-400/" title="View all posts in Expert (400)">
      <span property="articleSection">
       Expert (400)
      </span>
     </a>
     ,
     <a href="https://aws.amazon.com/blogs/database/category/post-types/technical-how-to/" title="View all posts in Technical How-to">
      <span property="articleSection">
       Technical How-to
      </span>
     </a>
    </span>
    |
    <a href="https://aws.amazon.com/blogs/database/cost-effective-bulk-processing-with-amazon-dynamodb/" property="url">
     Permalink
    </a>
    |
    <a href="https://aws.amazon.com/blogs/database/cost-effective-bulk-processing-with-amazon-dynamodb/#Comments">
     <i class="icon-comment">
     </i>
     Comments
    </a>
    |
    <a data-share-dialog-toggle="" href="#" role="button">
     <span class="span icon-share">
     </span>
     Share
    </a>
    <div class="blog-share-dialog" data-share-dialog="" style="display: none;">
     <ul>
      <li>
       <a aria-label="Share on Facebook" class="lb-txt" href="https://www.facebook.com/sharer/sharer.php?u=https://aws.amazon.com/blogs/database/cost-effective-bulk-processing-with-amazon-dynamodb/" rel="noopener noreferrer" target="_blank">
        <span class="icon-facebook-square">
        </span>
       </a>
      </li>
      <li>
       <a aria-label="Share on Twitter" class="lb-txt" href="https://twitter.com/intent/tweet/?text=Cost-effective%20bulk%20processing%20with%20Amazon%20DynamoDB&amp;via=awscloud&amp;url=https://aws.amazon.com/blogs/database/cost-effective-bulk-processing-with-amazon-dynamodb/" rel="noopener noreferrer" target="_blank">
        <span class="icon-twitter-square">
        </span>
       </a>
      </li>
      <li>
       <a aria-label="Share on LinkedIn" class="lb-txt" href="https://www.linkedin.com/shareArticle?mini=true&amp;title=Cost-effective%20bulk%20processing%20with%20Amazon%20DynamoDB&amp;source=Amazon%20Web%20Services&amp;url=https://aws.amazon.com/blogs/database/cost-effective-bulk-processing-with-amazon-dynamodb/" rel="noopener noreferrer" target="_blank">
        <span class="icon-linkedin-square">
        </span>
       </a>
      </li>
      <li>
       <a aria-label="Share on Email" class="lb-txt" href="mailto:?subject=Cost-effective%20bulk%20processing%20with%20Amazon%20DynamoDB&amp;body=Cost-effective%20bulk%20processing%20with%20Amazon%20DynamoDB%0A%0Ahttps://aws.amazon.com/blogs/database/cost-effective-bulk-processing-with-amazon-dynamodb/" rel="noopener noreferrer" target="_blank">
        <span class="icon-envelope-square">
        </span>
       </a>
      </li>
      <li class="blog-share-dialog-url">
       <input data-share-dialog-url="" readonly="" title="Link to Cost-effective bulk processing with Amazon DynamoDB" type="text" value="https://aws.amazon.com/blogs/database/cost-effective-bulk-processing-with-amazon-dynamodb/"/>
      </li>
     </ul>
    </div>
   </footer>
   <section class="blog-post-content lb-rtxt" property="articleBody">
    <p>
     Your
     <a href="https://aws.amazon.com/dynamodb/" rel="noopener" target="_blank">
      Amazon DynamoDB
     </a>
     table might store millions, billions, or even trillions of items. If you ever need to perform a bulk update action against items in a large table, it’s important to consider the cost. In this post, I show you three techniques for cost-effective in-place bulk processing with DynamoDB.
    </p>
    <h2>
     Characteristics of bulk processing
    </h2>
    <p>
     You might have several reasons for bulk processing:
    </p>
    <ul>
     <li>
      To delete outdated items for compliance reasons or to save ongoing storage costs.
      <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html" rel="noopener" target="_blank">
       DynamoDB Time to Live (TTL)
      </a>
      provides a zero-cost deletion capability targeted at use cases like this, but the TTL attribute must already exist on each item to be deleted. Perhaps your outdated items were loaded without a TTL attribute.
     </li>
     <li>
      To backfill an appropriate TTL attribute value on more recent items so that they will be automatically deleted according to the proper schedule going forward.
     </li>
     <li>
      To backfill a new
      <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html" rel="noopener" target="_blank">
       global secondary index (GSI)
      </a>
      partition key or sort key with a net-new attribute. For example, if the GSI sort key needs to be a combination of state and city (
      <code>
       &lt;state&gt;#&lt;city&gt;
      </code>
      ), that requires placing a new attribute onto every item.
     </li>
     <li>
      To apply any bulk changes where it’s not necessary for the work to be completed immediately.
     </li>
    </ul>
    <p>
     A common characteristic of bulk processing is that you don’t necessarily need it done right away. Often, bulk requests can be delayed to the next second, the next hour, or even the next day. This allows for innovative designs to accomplish the work at lower cost than if everything had to be performed immediately.
    </p>
    <p>
     For this post, I’m going to assume you want to do in-place no-downtime bulk updates on a table that’s also accepting organic (customer-driven) traffic. I’m also going to assume you have a truly large bulk update task, counted in the billions of items. If your bulk update task is smaller than this, the cost will be so low (no matter how you perform the work) that it doesn’t require the more careful approaches laid out here.
    </p>
    <h2>
     Large-scale pricing example
    </h2>
    <p>
     Imagine you have a truly large table: 500 billion items, each 500 bytes in size, for a total table size of 250 TB. This data has accumulated over the last 36 months and now you’d like to delete everything with a timestamp attribute older than 13 months. That’s 319 billion items you need to delete. Imagine furthermore you’d like to
     <a href="https://aws.amazon.com/blogs/database/part-2-backfilling-an-amazon-dynamodb-time-to-live-attribute-using-amazon-emr/" rel="noopener" target="_blank">
      add a TTL attribute
     </a>
     to the other 181 billion items so they’ll also be automatically deleted at 13 months of age going forward.
    </p>
    <table border="1px" cellpadding="10px">
     <tbody>
      <tr>
       <td>
        <strong>
         Metric
        </strong>
       </td>
       <td>
        <strong>
         Quantity
        </strong>
       </td>
      </tr>
      <tr>
       <td>
        Item count
       </td>
       <td>
        500 billion
       </td>
      </tr>
      <tr>
       <td>
        Item size
       </td>
       <td>
        500 bytes
       </td>
      </tr>
      <tr>
       <td>
        Table size
       </td>
       <td>
        250 TB
       </td>
      </tr>
      <tr>
       <td>
        Items to delete
       </td>
       <td>
        319 billion
       </td>
      </tr>
      <tr>
       <td>
        Items to update
       </td>
       <td>
        181 billion
       </td>
      </tr>
     </tbody>
    </table>
    <h3>
     Cost to modify all items using on-demand mode
    </h3>
    <p>
     The cost of writing (either deleting or updating) 500 billion small items depends on the table mode. In on-demand mode, it’s going to take exactly 500 billion write request units (WRUs). There’s no advantage with on-demand in controlling the timing of the requests (except to spread the work across the table to avoid hot partitions). Here is how that cost calculation breaks down for the
     <code>
      us-east-1
     </code>
     AWS Region for a table in on-demand mode:
    </p>
    <p>
     <strong>
      500 billion WRUs priced at $1.25 per million = $625,000
     </strong>
    </p>
    <p>
     There are also read costs to consider. A
     <code>
      Scan
     </code>
     can pull multiple items at a time. Using eventually consistent reads, it can pull sixteen 500-byte items per consumed read unit. That means scanning all 500 billion items will require about 31.25 billion read request units (RRUs).
    </p>
    <p>
     <strong>
      31.25 billion RRUs priced at $0.25 per million = $7,810
     </strong>
    </p>
    <p>
     The read costs are inconsequential compared to writes, so this post will focus on optimizing the timing of the writes.
    </p>
    <table border="1px" cellpadding="10px">
     <tbody>
      <tr>
       <td>
        <strong>
         Action
        </strong>
       </td>
       <td>
        <strong>
         Quantity needed
        </strong>
       </td>
       <td>
        <strong>
         Cost per
        </strong>
       </td>
       <td>
        <strong>
         Total cost
        </strong>
       </td>
      </tr>
      <tr>
       <td>
        Write 500 billion
        <br/>
        500-byte items
       </td>
       <td>
        500 billion WRUs
       </td>
       <td>
        $1.25 per million
       </td>
       <td>
        $625,000
       </td>
      </tr>
      <tr>
       <td>
        Scan 500 billion
        <br/>
        500-byte items
       </td>
       <td>
        31.25 billion RRUs
       </td>
       <td>
        $0.25 per million
       </td>
       <td>
        $7,810
       </td>
      </tr>
     </tbody>
    </table>
    <p>
     That’s about 790,000 item updates for every $1 of cost. Because the workload is flat and predictable, we can do much better using provisioned mode.
    </p>
    <h3>
     Cost to modify all items using provisioned mode
    </h3>
    <p>
     In provisioned mode, the pricing calculation is a little more complicated. This is because the bulk writes and the organic writes have to coexist in a way that doesn’t cause the table to be throttled. It’s in that coexisting relationship that we’ll find savings later in this post.
    </p>
    <p>
     Provisioned tables that handle organic traffic tend to have autoscaling enabled. Autoscaling moves the amount being provisioned in response to the amount being consumed, aiming to keep the provisioned amount sufficiently above the consumed amount that temporary spikes stay within the provisioned amount.
    </p>
    <p>
     Autoscaling settings include a minimum value (don’t ever scale below this), maximum value (don’t ever scale above this), and a target utilization percentage (aim to consume equal to this much of provisioned, with a default value of 70 percent). The target utilization effectively controls how much padding goes above the consumed amount to determine the provisioned amount. Higher numbers provide less padding (increasing the risk of throttling).
    </p>
    <p>
     A steady bulk job consuming 1,000,000 WCUs will run for 500,000 seconds (139 hours) to process all 500 billion items. During that time, autoscaling will detect the increased traffic and provision an additional 1,428,500 WCUs (1,000,000 WCU divided by the 70 percent target) on top of whatever was needed for the organic traffic.
    </p>
    <p>
     Here is what the cost calculation looks like in us-east-1 for a table in provisioned capacity mode:
    </p>
    <p>
     <strong>
      1,428,500 WCUs * 139 hours * $0.00065 per WCU-hour = $129,065
     </strong>
    </p>
    <p>
     The relatively minor cost of the scan is included in the following table.
    </p>
    <table border="1px" cellpadding="10px">
     <tbody>
      <tr>
       <td>
        <strong>
         Action
        </strong>
       </td>
       <td>
        <strong>
         Quantity consumed
        </strong>
       </td>
       <td>
        <strong>
         Quantity provisioned
        </strong>
       </td>
       <td>
        <strong>
         Cost per
        </strong>
       </td>
       <td>
        <strong>
         Total cost
        </strong>
       </td>
      </tr>
      <tr>
       <td>
        Write 500 billion
        <br/>
        500-byte items
       </td>
       <td>
        1,000,000 WCUs
        <br/>
        for 139 hours
       </td>
       <td>
        1,428,500 WCUs
        <br/>
        for 139 hours
       </td>
       <td>
        $0.00065 per WCU-hour
       </td>
       <td>
        $129,065
       </td>
      </tr>
      <tr>
       <td>
        Scan 500 billion
        <br/>
        500-byte items
       </td>
       <td>
        62,500 RRUs
        <br/>
        for 139 hours
       </td>
       <td>
        89,285 RCUs
        <br/>
        for 139 hours
       </td>
       <td>
        $0.00013 per RCU-hour
       </td>
       <td>
        $1,614
       </td>
      </tr>
     </tbody>
    </table>
    <p>
     That’s about 3,826,000 item updates for every $1 of cost, a significant cost reduction from on-demand, as you’d expect for workloads that don’t need the instant reactiveness of on-demand mode. On-demand mode can actually be lower cost if the traffic is spiky, but perfectly flat work prices well with provisioned, as this shows.
    </p>
    <p>
     Let’s now look at three different techniques that use timing to control when and at what rate the bulk write work is performed in order to achieve free or reduced-cost in-place updates.
    </p>
    <h2>
     Use unused reserved capacity
    </h2>
    <p>
     If you’ve purchased
     <a href="https://aws.amazon.com/dynamodb/reserved-capacity/" rel="noopener" target="_blank">
      reserved capacity
     </a>
     , you can potentially use it to provide free bulk writes by timing the writes during the periods when you have unused reserved capacity.
    </p>
    <p>
     Reserved capacity (RC) provides a discount in exchange for a one- or three-year commitment to provision a minimum amount of throughput. If you have daily traffic varying from 300,000 WCUs to 700,000 WCUs, the right amount of one-year RC to purchase is somewhere around 500,000 WCUs. Buying only at the low point misses out on savings for all the workload above baseline; buying at peak over-provisions for much of the day; buying somewhere around the midpoint tends to be best.
    </p>
    <p>
     This means that any time when organic traffic is between 300,000 and 500,000, a carefully constructed bulk job executor could fill the gap and perform the bulk work at zero extra cost. If you’ve already committed to a 500,000 minimum, you might as well fill the slow period with bulk work.
    </p>
    <p>
     The following chart visualizes this. The spiky orange line is the organic consumed capacity, fluctuating up and down throughout the day. The horizontal red line is the RC purchased amount. The gap below the horizontal purchased line and above the spiky consumed line is the opportunity for free writes.
    </p>
    <div class="wp-caption alignnone" id="attachment_34530" style="width: 987px">
     <img alt="Figure 1: Time writes to match periods with unused reserved capacity" aria-describedby="caption-attachment-34530" class="wp-image-34530 size-full" height="422" src="https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2023/05/20/DBBLOG-3003-Figure1.png" style="margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC" width="977"/>
     <p class="wp-caption-text" id="caption-attachment-34530">
      Figure 1: Time writes to match periods with unused reserved capacity
     </p>
    </div>
    <p>
     Imagine a bulk executor parameterized to consume a certain amount of self-throttled capacity during particular hours in the day, based on historic consumption norms. For example, if there are usually 200,000 reserved-but-unused WCUs during the middle of the night, the bulk executor can know to consume those WCUs for itself. Processing at a rate of 200,000 items per second would touch all 500 billion items in 694 hours. Assuming an average of 30 hours per week with this level of unused RC, it would take 13 weeks to process the backlog.
    </p>
    <p>
     Calculating the historic norms is straightforward if you have only a single table but takes extra work in a larger organization. That’s because reserved capacity isn’t assigned to a single table but instead is shared among all tables for a given Region in all accounts connected to the same payer account. It’s not the low periods on just one table that matter, it’s the low periods across all tables, and then calculating what RC is unused during those periods. That’s the run rate to give to the bulk executor.
    </p>
    <p>
     Autoscaling introduces complexity to this design. If you leave autoscaling at the 70 percent default target, you can only actually consume 140,000 WCUs because that’s the write level that will result in all 200,000 spare WCUs being provisioned to fill the utilization gap.
    </p>
    <p>
     What you can do then is, during the hours of bulk execution, tighten the autoscaling settings to the most aggressive possible value of 90 percent. This greatly cuts down the padding and lets you consume up to 180,000 WCUs to reach the 200,000 WCUs provisioned.
    </p>
    <p>
     Normally, if you set an aggressive target utilization, it increases the risk of throttling during any organic traffic spikes. You can mitigate this by having the bulk executor quickly and harshly self-throttle when it receives a
     <code>
      ProvisionedThroughputExceededException
     </code>
     so that it stops its own throughput consumption for a short while to make way for organic traffic. The padding is still effectively there, just being opportunistically consumed by a bulk executor job that converts itself to padding as needed. Picture a safety circular saw encountering a test hot dog and abruptly stopping itself to minimize any damage. There’s less need for conservatively autoscaling padding if the bulk job can shed its consumption quickly. Make sure to adjust the SDK retry settings to avoid automatic retries when implementing a harsh self-throttle.
    </p>
    <p>
     This approach, when available, can perform all the updates over time without additional service charges, but it does require a fixed amount of engineering effort up front.
    </p>
    <h2>
     Tighten autoscaling
    </h2>
    <p>
     If you don’t happen to have unused RC but are using provisioned mode with autoscaling, tightening autoscaling provides another way to achieve nearly free bulk writes.
    </p>
    <p>
     As discussed at the end of the previous section, the goal of autoscaling target utilization is to keep the provisioned capacity line sufficiently above the consumed capacity line that short bursts of consumption don’t exceed the provisioned amount. A higher target leaves less padding, while a lower target leaves more padding. The following figure shows how autoscaling works. The spikey orange line is the consumed WCU and the flat blue lines are the provisioned WCU.
    </p>
    <div class="wp-caption alignnone" id="attachment_34531" style="width: 987px">
     <img alt="Figure 2: Consumed and provisioned WCU with autoscaling" aria-describedby="caption-attachment-34531" class="wp-image-34531 size-full" height="435" loading="lazy" src="https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2023/05/20/DBBLOG-3003-Figure2.png" style="margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC" width="977"/>
     <p class="wp-caption-text" id="caption-attachment-34531">
      Figure 2: Consumed and provisioned WCU with autoscaling
     </p>
    </div>
    <p>
     The technique then is to raise the target (which lowers the flat provisioned lines), add in a steady proportional percentage of bulk work on top of the organic traffic (raising the spiky line a little bit and putting the provisioned line back where it was originally), and have the bulk work self-throttle if encountering table-level throttles (making it acceptable to have the more aggressive target). The bulk work gets done at a rate proportional to some percentage of organic traffic without additional cost.
    </p>
    <p>
     The following chart visualizes this. The lower spiky orange line is the same organic traffic, straight blue lines are the same provisioned amount, and the upper spiky orange line is the consumed capacity with the bit of bulk work added on. Raising the target utilization keeps the straight lines in place even with the higher spiky line.
    </p>
    <div class="wp-caption alignnone" id="attachment_34532" style="width: 987px">
     <img alt="Figure 3: Tighten target utilization and add throttle-sensitive bulk work" aria-describedby="caption-attachment-34532" class="wp-image-34532 size-full" height="435" loading="lazy" src="https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2023/05/20/DBBLOG-3003-Figure3.png" style="margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC" width="977"/>
     <p class="wp-caption-text" id="caption-attachment-34532">
      Figure 3: Tighten target utilization and add throttle-sensitive bulk work
     </p>
    </div>
    <p>
     Having a higher target utilization value allows less room for spikes in traffic. That’s why you wouldn’t naturally have the target any higher than is safe. The overly high value is acceptable here because the bulk work can severely self-throttle (as covered previously) whenever it receives a throughput exceeded exception, freeing up the throughput immediately for the organic traffic and making the full distance between the orange and blue lines available for organic spikes.
    </p>
    <p>
     The bulk executor operates at a fixed percentage of the observed organic traffic to fill the gap opened by the higher target utilization. When changing the target from 70 percent to 80 percent, it opens the bulk job to consume approximately 15 percent of organic traffic as it rises and falls, which mathematically keeps the provisioned line at roughly the same levels as would have been consumed originally.
    </p>
    <p>
     Here’s the math confirming that changing 70 percent to 80 percent allows for a 15 percent increase:
    </p>
    <ul>
     <li>
      Assume 500,000 WCUs of fluctuating organic traffic.
     </li>
     <li>
      A 70 percent target results in 500,000/0.7 = 714,285 WCUs provisioned by autoscaling.
     </li>
     <li>
      Change it to an 80 percent target and it’s 500,000/0.8 = 625,000 WCUs.
     </li>
     <li>
      Add 15 percent of bulk work changing 500,000 to 575,000 and it’s now 575,000/0.8 = 718,750 WCUs provisioned. That’s right about where we started.
     </li>
    </ul>
    <p>
     The exact math is (80/70) – 1 = 14.3%. If you were to move a 50 percent target up to 70 percent you can add (70/50) – 1 = 40% of bulk work.
    </p>
    <p>
     Adding a fluctuating but average rate of 75,000 WCUs to the organic traffic will complete the 500 billion item backlog in 11 weeks.
    </p>
    <p>
     The challenge of this design is in keeping the bulk work running at a steady percentage of the organic traffic, which can vary. The bulk executor must observe the
     <a href="https://aws.amazon.com/cloudwatch/" rel="noopener" target="_blank">
      Amazon CloudWatch
     </a>
     metrics so as to detect the (few minutes delayed) overall consumed capacity, subtract its own recent history, mathematically determine the organic usage level, and adjust its self-throttle to run as a fixed percentage of that organic traffic. It’s hard to be precise, which is why it’s best to not be too aggressive in the new utilization target. Prefer 80 percent over 90 percent, for example.
    </p>
    <p>
     One extra challenge: it’s not possible to differentiate between a table-level throughput exceeded exception and a single hot partition causing a throughput exceeded exception. The bulk job should err on the side of harshly self-throttling to avoid throttling organic traffic in either case. It’s important that the bulk job evenly spread its writes across partitions, as we’ll discuss later, to minimize how often the bulk executor has to pause due to one hot partition.
    </p>
    <p>
     This approach, like the previous, can perform all the updates over time without additional service charges, but with a fixed amount of engineering effort up front.
    </p>
    <h2>
     Remove autoscaling
    </h2>
    <p>
     If you’re using provisioned capacity and want to control how long the bulk executor should take, or if you prefer a design with fewer moving parts, removing autoscaling provides cost-effective but not quite free bulk processing.
    </p>
    <p>
     What you do is turn off autoscaling, set your table to a fixed amount of provisioned write capacity (somewhere above peak organic traffic), and have the bulk executor fill in the gap between the organic traffic and the provisioned level, being sure to set the bulk executor to self-throttle itself to ensure organic traffic isn’t throttled.
    </p>
    <p>
     The cost savings comes from the fact that, during the duration of the bulk processing time, the normal padding required of autoscaling isn’t needed. If the bulk processing runs for 10 weeks, any padding that would have been required during those weeks can be subtracted from the cost of the bulk processing. Unused padding becomes useful bulk processing time.
    </p>
    <p>
     The bulk executor needs to track its own consumption, observe in CloudWatch the overall consumption, and self-throttle to keep the overall consumption close to the provisioned level. It also needs to severely self-throttle should it encounter a throughput exceeded exception.
    </p>
    <p>
     The consumption goal target should be a little below 100 percent to keep the benefits of burst capacity to handle unanticipated organic write spikes. Burst capacity allows a table to run above its provisioned capacity for short durations. Burst capacity is given on a best-effort basis, but when available helps your table avoid throttles if there’s an organic spike; instead, you’ll see throttles only after the burst capacity has run out.
    </p>
    <p>
     The amount available to burst is 300 times the per-second provisioned capacity. As an example of what that means, a table provisioned at 200,000 WCUs can potentially burst at 300,000 for 10 minutes or 250,000 for 20 minutes before encountering throttles at the table level (individual partitions have no burst capacity). The burst capacity is replenished as your traffic spends time below provisioned capacity.
    </p>
    <p>
     Providing a gap of 5 percent below full consumption means that after 100 minutes at that level, the burst capacity allowance would be fully restored after any total depletion, so aiming for 95 percent consumption is a good compromise between speed, cost effectiveness, and keeping burst availability. If you anticipate more spikes in the organic workload, you can leave a larger percentage of gap.
    </p>
    <p>
     The following chart visualizes this. Spiky orange is the organic consumed capacity, flat blue the typical provisioned amount, and dotted red the new fixed provisioned capacity. The bulk work should run a bit below the flat red line.
    </p>
    <div class="wp-caption alignnone" id="attachment_34533" style="width: 987px">
     <img alt="Figure 4: Turn off auto-scaling and use bulk work to achieve flat consumption" aria-describedby="caption-attachment-34533" class="wp-image-34533 size-full" height="422" loading="lazy" src="https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2023/05/20/DBBLOG-3003-Figure4.png" style="margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC" width="977"/>
     <p class="wp-caption-text" id="caption-attachment-34533">
      Figure 4: Turn off auto-scaling and use bulk work to achieve flat consumption
     </p>
    </div>
    <p>
     This approach subtracts the cost inherent in the overhead of auto-scaling (for example, with a 70 percent target utilization, 30 percent of the cost is devoted just to padding) and achieves around 95 percent utilization for organic writes as well as bulk writes.
    </p>
    <p>
     For the 500 billion item table, let’s assume you want to provision at a fixed 800,000 WCUs because that’s above any expected peak in the usual 300,000 to 700,000 WCUs of fluctuating organic traffic. The bulk traffic will fill in the gap between the organic traffic and the target of 760,000 WCUs (respecting that 5 percent gap). The rate of bulk traffic will vary but since the organic traffic averages 500,000 WCUs the bulk work will average 260,000 WCUs. At that write rate, it will process all 500 billion items in three weeks.
    </p>
    <p>
     Let’s consider costs. During those three weeks, if there was no bulk activity, we would expect an average of 500,000 WCUs of organic traffic and, with a 70 percent target utilization, that would produce an average of 714,285 WCUs provisioned. Instead, we opted to provision a fixed 800,000 WCUs. For three weeks we ran the bulk executor at an average rate of 260,000 WCUs while only adding an incremental cost of about 85,000 WCUs. That means the bulk work was done at a 67 percent cost reduction compared to fully utilized provisioned writes and an even greater cost reduction compared to an autoscaling table.
    </p>
    <p>
     At the beginning of this post, we calculated a bulk job against a provisioned table would cost $129,065. This approach consumes an average of 85,000 WCUs across three weeks for a total cost of:
    </p>
    <p>
     <strong>
      85,000 WCUs * 21 days * 24 hours * $0.00065 per WCU-hour = $27,846
     </strong>
    </p>
    <p>
     That’s about 18,000,000 item updates per $1 of cost.
    </p>
    <h2>
     Comparing approaches
    </h2>
    <p>
     The first approach—use unused reserved capacity—is straightforward and provides potentially free bulk writes, but only works to the extent that you have repeating and predictable periods of underutilized reserved capacity. The longer and deeper the periods, the faster the bulk work can complete.
    </p>
    <p>
     The second approach—tighten autoscaling—is more complex. It achieves potentially free bulk writes, with some risk of increased throttling or of the bulk work getting in the way of autoscaling based on the organic write traffic. The risk can be controlled by parameterizing the percentage of bulk work added to the organic traffic.
    </p>
    <p>
     The third approach—remove autoscaling—is straightforward, should execute the bulk work faster than the second option, but will generally incur some cost for the bulk work as the single provisioned capacity line will probably be above the average of the autoscaling line. However, the cost is in your control as you choose the fixed provisioned amount and you can save the cost of the padded throughput during the time of the bulk work.
    </p>
    <table border="1px" cellpadding="10px">
     <tbody>
      <tr>
       <td>
        <span style="color: #ffffff">
         .
        </span>
       </td>
       <td>
        <strong>
         Use unused RC
        </strong>
       </td>
       <td>
        <strong>
         Tighten autoscaling
        </strong>
       </td>
       <td>
        <strong>
         Remove autoscaling
        </strong>
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         Summary
        </strong>
       </td>
       <td>
        Convert unused RC into used bulk work during natural lulls in RC usage
       </td>
       <td>
        Raise the autoscaling target and add bulk work (that temporarily halts itself to make way for organic traffic spikes) to make the high target acceptable
       </td>
       <td>
        Provision a high fixed amount and add bulk work (that temporarily halts itself to make way for organic traffic spikes) to achieve near full utilization
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         Cost
        </strong>
       </td>
       <td>
        Free
       </td>
       <td>
        Free
       </td>
       <td>
        Low
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         Advantages
        </strong>
       </td>
       <td>
        Simplest design
       </td>
       <td>
        Works even without RC
       </td>
       <td>
        You control the execution speed; straightforward design
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         Disadvantages
        </strong>
       </td>
       <td>
        Must be using RC and have predictable lulls
       </td>
       <td>
        Most complex design
       </td>
       <td>
        Incurs a cost
       </td>
      </tr>
     </tbody>
    </table>
    <h2>
     Spread the write load using a shuffled scan
    </h2>
    <p>
     Whichever approach you choose, you’ll want to spread the write load across the table’s partitions, which requires some extra care if you’re driving writes from a table scan.
    </p>
    <p>
     When doing repeated
     <code>
      Scan
     </code>
     calls, you pull a page at a time. Each call returns the number of items you specify as a limit, up to a maximum of 1 MB. If your call hasn’t reached the end, its response includes a
     <code>
      LastEvaluatedKey
     </code>
     , which you pass into the next
     <code>
      Scan
     </code>
     call to pull the next page. Internally, the
     <code>
      Scan
     </code>
     call pulls items from one partition, then the next.
    </p>
    <p>
     If you do what seems natural and drive your writes directly from the items returned by the
     <code>
      Scan
     </code>
     , you’ll be sending your writes toward whatever partition the
     <code>
      Scan
     </code>
     is working against at that point, creating a rolling hot partition and limiting your write rate to around 1,000 WCUs per second (the fixed write limit of one partition).
    </p>
    <p>
     To avoid this, you want to do a shuffled scan:
    </p>
    <ol>
     <li>
      Initiate a
      <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan" rel="noopener" target="_blank">
       parallel scan
      </a>
      .
     </li>
     <li>
      Specify you want the scan to use for example 10,000 segments.
     </li>
     <li>
      Pull 100 items from a segment (using limit), then pull 100 from another segment chosen at random.
     </li>
     <li>
      Keep track of the
      <code>
       LastEvaluatedKey
      </code>
      for each segment number so you can reuse it when you come back to that segment again to read the next block.
     </li>
     <li>
      When a segment doesn’t return a
      <code>
       LastEvaluatedKey
      </code>
      anymore, you know that segment is fully read and you don’t read from it again.
     </li>
     <li>
      When all segments are done, the full table will have been scanned in shuffled order.
     </li>
    </ol>
    <p>
     This pulls 100 items at a time from 10,000 different segments of the table, effectively jumping here and there to create a work list that’s well distributed across the table.
    </p>
    <p>
     If you employ parallel workers, you can give each worker a random set of segments for it to shuffle against. That allows each worker to spread its activity across the table.
    </p>
    <h2>
     Summary
    </h2>
    <p>
     Bulk writes—be they deletes, inserts, or updates—have the unique characteristic that they can be performed at a time and at a rate that’s chosen for cost-effectiveness.
    </p>
    <p>
     If you have reserved capacity, it’s possible to obtain writes at no extra charge by making those writes when the organic traffic levels are below the RC minimums.
    </p>
    <p>
     If you’re using autoscaling, it’s possible to obtain free or nearly-free writes by raising the target utilization percentage and filling the gap with bulk writes, preventing throttling issues by having the bulk job harshly self-throttle whenever it reaches a throughput limit.
    </p>
    <p>
     If you want a simple design or faster processing, you can obtain low-cost writes by setting a fixed provisioned WCU amount (no autoscaling) and have the bulk work consume capacity almost to that fixed amount. The cost savings come from removing the usual cost of the autoscaling overhead during the time of the bulk work.
    </p>
    <p>
     Remember that when driving any bulk writes from a table scan, it’s important to spread the scan calls across the table so as to spread the write calls as well, thereby avoiding hot partitions. You can do that by using a shuffled scan (built on a parallel scan) that pulls a small number of items repeatedly from random segments.
    </p>
    <p>
     If you have any comments or questions, leave a comment in the comments section. You can find more
     <a href="https://aws.amazon.com/blogs/database/category/database/amazon-dynamodb/" rel="noopener" target="_blank">
      DynamoDB posts
     </a>
     and others posts written by
     <a href="https://aws.amazon.com/blogs/database/author/jasonhunter/" rel="noopener" target="_blank">
      Jason Hunter
     </a>
     in the
     <a href="https://aws.amazon.com/blogs/database/category/database/amazon-dynamodb/" rel="noopener" target="_blank">
      AWS Database Blog
     </a>
     .
    </p>
    <hr/>
    <h3>
     About the Author
    </h3>
    <p>
     <strong>
      <img alt="" class="size-full wp-image-31600 alignleft" height="149" loading="lazy" src="https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2023/03/22/DBBLOG-2281-jzhunter-headshot-1.png" width="100"/>
      Jason Hunter
     </strong>
     is a California-based Principal Solutions Architect specializing in DynamoDB. He’s been working with NoSQL Databases since 2003. He’s known for his contributions to Java, open source, and XML.
    </p>
    <!-- '"` -->
   </section>
   <footer>
   </footer>
   <aside class="blog-comments">
    <h2 id="Comments">
     Comments
    </h2>
    <div class="blog-commenting" data-comment-url="https://commenting.awsblogs.com/embed-1.0.html?disqus_shortname=aws-databaseblog&amp;disqus_identifier=34529&amp;disqus_title=Cost-effective+bulk+processing+with+Amazon+DynamoDB&amp;disqus_url=https://aws.amazon.com/blogs/database/cost-effective-bulk-processing-with-amazon-dynamodb/" data-lb-comp="aws-blog:commenting" data-origin="https://commenting.awsblogs.com" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin">
     <p>
      <a href="https://commenting.awsblogs.com/embed-1.0.html?disqus_shortname=aws-databaseblog&amp;disqus_identifier=34529&amp;disqus_title=Cost-effective+bulk+processing+with+Amazon+DynamoDB&amp;disqus_url=https://aws.amazon.com/blogs/database/cost-effective-bulk-processing-with-amazon-dynamodb/" property="discussionUrl">
       View Comments
      </a>
     </p>
    </div>
   </aside>
   <div class="js-mbox" data-mbox="en_blog_post_comments">
   </div>
  </article>
 </main>
 <div class="blog-sidebar lb-col lb-tiny-24 lb-mid-8">
  <div class="awsm">
   <div class="lb-box" style="padding-left:30px;">
    <h3 class="lb-txt-none lb-h3 lb-title" id="Resources" style="margin-top:30px;">
     Resources
    </h3>
    <div class="data-attr-wrapper lb-none-pad lb-none-v-margin lb-box" data-da-campaign="acq_awsblogsb" data-da-channel="ha" data-da-content="database-resources" data-da-language="en" data-da-placement="blog-sb-resources" data-da-trk="blog_initial" data-da-type="ha">
     <ul class="lb-txt-none lb-ul lb-list-style-none lb-li-none-v-margin lb-tiny-ul-block">
      <li>
       <a href="https://aws.amazon.com/getting-started?sc_ichannel=ha&amp;sc_icampaign=acq_awsblogsb&amp;sc_icontent=database-resources">
        Getting Started
       </a>
      </li>
      <li>
       <a href="https://aws.amazon.com/new?sc_ichannel=ha&amp;sc_icampaign=acq_awsblogsb&amp;sc_icontent=database-resources">
        What's New
       </a>
      </li>
     </ul>
     <div class="lb-grid lb-row lb-row-max-large lb-snap">
      <div class="lb-col lb-tiny-24 lb-mid-8">
       <hr class="lb-divider"/>
      </div>
      <div class="lb-col lb-tiny-24 lb-mid-8">
      </div>
      <div class="lb-col lb-tiny-24 lb-mid-8">
      </div>
     </div>
     <h3 class="lb-txt-none lb-h3 lb-title" id="Blog_Topics" style="margin-top:30px;">
      Blog Topics
     </h3>
     <div class="data-attr-wrapper lb-none-pad lb-none-v-margin lb-box" data-da-campaign="acq_awsblogsb" data-da-channel="ha" data-da-content="database-social" data-da-language="en" data-da-placement="blog-sb-social" data-da-trk="blog_initial" data-da-type="ha">
      <ul class="lb-txt-none lb-ul lb-list-style-none lb-li-none-v-margin lb-tiny-ul-block">
       <li>
        <a href="https://aws.amazon.com/blogs/database/category/database/amazon-aurora/">
         Amazon Aurora
        </a>
       </li>
       <li>
        <a href="https://aws.amazon.com/blogs/database/category/database/amazon-document-db/">
         Amazon DocumentDB
        </a>
       </li>
       <li>
        <a href="https://aws.amazon.com/blogs/database/category/database/amazon-dynamodb/">
         Amazon DynamoDB
        </a>
       </li>
       <li>
        <a href="https://aws.amazon.com/blogs/database/category/database/amazon-elasticache/">
         Amazon ElastiCache
        </a>
       </li>
       <li>
        <a href="https://aws.amazon.com/blogs/database/category/database/amazon-managed-apache-cassandra-service/">
         Amazon Keyspaces (for Apache Cassandra)
        </a>
       </li>
       <li>
        <a href="https://aws.amazon.com/blogs/database/category/blockchain/amazon-managed-blockchain/">
         Amazon Managed Blockchain
        </a>
       </li>
       <li>
        <a href="https://aws.amazon.com/blogs/database/category/database/amazon-memorydb-for-redis/">
         Amazon MemoryDB for Redis
        </a>
       </li>
       <li>
        <a href="https://aws.amazon.com/blogs/database/category/database/amazon-neptune/">
         Amazon Neptune
        </a>
       </li>
       <li>
        <a href="https://aws.amazon.com/blogs/database/category/database/amazon-quantum-ledger-database/">
         Amazon Quantum Ledger Database (Amazon QLDB)
        </a>
       </li>
       <li>
        <a href="https://aws.amazon.com/blogs/database/category/database/amazon-rds/">
         Amazon RDS
        </a>
       </li>
       <li>
        <a href="https://aws.amazon.com/blogs/database/category/database/amazon-timestream/">
         Amazon Timestream
        </a>
       </li>
       <li>
        <a href="https://aws.amazon.com/blogs/database/category/database/aws-database-migration-service/">
         AWS Database Migration Service
        </a>
       </li>
       <li>
        <a href="https://aws.amazon.com/blogs/database/category/database/aws-schema-conversion-tool/">
         AWS Schema Conversion Tool
        </a>
       </li>
      </ul>
      <div class="lb-grid lb-row lb-row-max-large lb-snap">
       <div class="lb-col lb-tiny-24 lb-mid-8">
        <hr class="lb-divider"/>
       </div>
       <div class="lb-col lb-tiny-24 lb-mid-8">
       </div>
       <div class="lb-col lb-tiny-24 lb-mid-8">
       </div>
      </div>
     </div>
    </div>
    <h3 class="lb-txt-none lb-h3 lb-title" id="Follow">
     Follow
    </h3>
    <div class="data-attr-wrapper lb-none-pad lb-none-v-margin lb-box" data-da-campaign="acq_awsblogsb" data-da-channel="ha" data-da-content="database-social" data-da-language="en" data-da-placement="blog-sb-social" data-da-trk="blog_initial" data-da-type="ha">
     <ul class="lb-txt-none lb-ul lb-list-style-none lb-li-none-v-margin lb-tiny-ul-block">
      <li>
       <a href="https://twitter.com/awscloud">
        <i class="icon-twitter-square">
        </i>
        Twitter
       </a>
      </li>
      <li>
       <a href="https://www.facebook.com/amazonwebservices">
        <i class="icon-facebook-square">
        </i>
        Facebook
       </a>
      </li>
      <li>
       <a href="https://www.linkedin.com/company/amazon-web-services/">
        <i class="icon-linkedin-square">
        </i>
        LinkedIn
       </a>
      </li>
      <li>
       <a href="https://www.twitch.tv/aws">
        <i class="icon-twitch">
        </i>
        Twitch
       </a>
      </li>
      <li>
       <a href="https://pages.awscloud.com/communication-preferences?sc_ichannel=ha&amp;sc_icampaign=acq_awsblogsb&amp;sc_icontent=database-social">
        <i class="icon-envelope-square">
        </i>
        Email Updates
       </a>
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="js-mbox" data-mbox="en_blog_post_sidebar">
  </div>
 </div>
</div>
