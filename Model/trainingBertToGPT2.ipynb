{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# https://cs.stanford.edu/~zxie/textgen.pdf\n",
    "# https://www.tensorflow.org/text/tutorials/transformer#set_up_the_tokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, \\\n",
    "                        TextDataset, \\\n",
    "                        DataCollatorForLanguageModeling, \\\n",
    "                        Trainer, \\\n",
    "                        TrainingArguments,\\\n",
    "                        GPT2Tokenizer,\\\n",
    "                        GPT2Config,\\\n",
    "                        BertTokenizerFast,\\\n",
    "                        BertModel\n",
    "                        \n",
    "            \n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import boto3\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cpu')\n",
    "    print(\"GPU!!!!!!!!!!!!!!!!\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"CPU :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframe\n",
    "df = pd.read_csv(\"./data.csv\", delimiter = \"\\t\", names=['turkish','english'])\n",
    "df = df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['turkish', 'english'], dtype='object')\n",
      "                                             turkish  \\\n",
      "0  emekli üyeler kongre'nin şu sıralar çete savaş...   \n",
      "1  entellektüellik , klas , asalet veya hikaye il...   \n",
      "2  hangisi olduğunu tahmin edebildiniz mi ? şirke...   \n",
      "3  pek uzak yerlere seyahat edemez veya belli bir...   \n",
      "4                                 heyecanlanmıştım .   \n",
      "\n",
      "                                             english  \n",
      "0  retiring members nowadays say that it 's becom...  \n",
      "1  no sophistication , no class , no dignity , no...  \n",
      "2                     did you guess it ? companies .  \n",
      "3  you ca n't travel very far or venture too far ...  \n",
      "4                                    i was excited .  \n",
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "#verify that the dataset has been loaded\n",
    "\n",
    "print(df.columns)\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 31178, 117, 14796, 10301, 13028, 136, 102]\n",
      "[CLS] |\n",
      "Hello |\n",
      ", |\n",
      "how |\n",
      "are |\n",
      "you |\n",
      "? |\n",
      "[SEP] |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bert-base-uncased/tokenizer_config.json',\n",
       " './bert-base-uncased/special_tokens_map.json',\n",
       " './bert-base-uncased/vocab.txt',\n",
       " './bert-base-uncased/added_tokens.json',\n",
       " './bert-base-uncased/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "sentence = \"Hello, how are you?\"\n",
    "tokens = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "tokenizer.eos_token = '[PAD]'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokens)\n",
    "for token in tokens:\n",
    "    print(tokenizer.decode(token), '|')\n",
    "tokenizer.save_pretrained('./bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101,  75933,  12464,  11858, 101615,  10107,  23763,  10189,  10271,\n",
      "            112,    187,  13461,  11850,  16330,  74393,    119,    102,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "#tokenize the pandas dataframe\n",
    "df['english_tokens'] = df['english'].apply(lambda x: tokenizer.encode_plus(x, add_special_tokens=True, padding = 'max_length', max_length = 768, truncation = True, return_tensors = 'pt'))\n",
    "df['turkish_tokens'] = df['turkish'].apply(lambda x: tokenizer.encode_plus(x, add_special_tokens=True, padding = 'max_length', max_length = 768, truncation = True, return_tensors = 'pt'))\n",
    "\n",
    "print(df['english_tokens'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "encoder = bert_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodes the tokens\n",
    "# print(df['english_tokens'].values[0]['attention_mask'].squeeze(0))\n",
    "#TODO: fix the attention mask\n",
    "df['english_encoded'] = df['english_tokens'].apply(lambda x: encoder(x['input_ids'].to(torch.float32).unsqueeze(0)\n",
    "                                                                    #  attention_mask=x['attention_mask'].to(torch.float32).unsqueeze(0)\n",
    "                                                                     )[0].to(torch.float32)\n",
    "                                                                     )\n",
    "df['turkish_encoded'] = df['turkish_tokens'].apply(lambda x: encoder(x['input_ids'].to(torch.float32).unsqueeze(0)\n",
    "                                                                    #   attention_mask=x['attention_mask'].to(torch.float32).unsqueeze(0))[0]\n",
    "                                                                      )[0].long()\n",
    "                                                                      )\n",
    "#add the attention masks\n",
    "df['attention_masks'] = df['english_tokens'].apply(lambda x: x['attention_mask'].unsqueeze(0))\n",
    "df['decoder_attention_masks'] = df['turkish_tokens'].apply(lambda x: x['attention_mask'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-1.0371e+00,  3.9515e-01, -6.5867e-01, -7.2396e-02,  1.6084e+00,\n",
      "          -5.9844e-01, -7.1354e-01, -2.3577e-02,  7.6560e-01,  7.0373e-01,\n",
      "          -5.9151e-01,  5.9707e-01,  3.3662e-01,  2.8603e-01,  2.6011e-01,\n",
      "          -1.7486e-01, -5.4137e-01, -4.3599e-02,  8.5187e-01,  1.3756e+00,\n",
      "           1.8264e+00,  4.1690e-01, -4.3771e-01, -1.1941e-01, -6.6531e-01,\n",
      "          -4.7859e-02, -2.0685e-01, -1.0543e+00, -6.8715e-01,  1.4992e-01,\n",
      "           2.4614e-01,  3.4564e-01, -3.0112e-01, -4.2113e-02, -2.6306e-02,\n",
      "           8.2188e-01,  1.9872e-01,  7.0809e-02,  5.1735e-01,  1.7551e-01,\n",
      "           4.2995e-01,  5.8777e-01,  9.4538e-01,  2.2546e-01, -5.3248e-01,\n",
      "          -1.1268e+00,  3.6727e-01, -8.3205e-01, -1.7676e-01, -1.7672e-01,\n",
      "           2.2317e-03, -2.4613e-01,  7.3480e-01, -5.2762e-01,  3.1814e-01,\n",
      "           4.5334e-01,  1.5647e-02, -3.5377e-01,  4.6430e-01,  4.4255e-01,\n",
      "          -4.8431e-01, -1.2384e-01, -5.5225e-01, -3.6179e-01, -3.3160e-01,\n",
      "          -4.0192e-01,  2.2938e-01, -8.0931e-01, -5.6261e-01,  5.2370e-01,\n",
      "           5.6121e-01,  1.2388e-02, -9.8176e-01, -3.5730e-01, -9.0447e-01,\n",
      "           1.9532e+00,  8.7846e-01,  4.6020e-01,  1.0252e+00, -1.8131e-01,\n",
      "           1.1017e+00,  4.4659e-01,  3.7009e-01,  8.7765e-01, -9.4957e-01,\n",
      "          -5.6099e-01,  1.5106e-01, -1.3312e+00,  6.5580e-01, -3.7384e-01,\n",
      "           1.1190e+00, -4.0333e-01,  7.7124e-01,  8.2607e-02, -8.8927e-02,\n",
      "           5.7196e-01, -3.1273e-01, -2.9085e-01,  1.8818e-01, -2.6134e-01,\n",
      "          -8.0077e-01, -3.3680e-01,  4.9330e-01, -5.0678e-01,  8.1256e-01,\n",
      "          -3.3295e-01, -5.2752e-01, -1.0664e+00, -5.5914e-01, -1.2874e-01,\n",
      "          -4.1192e-01,  2.5329e-01, -4.9682e-01, -5.6379e-02,  6.1430e-01,\n",
      "          -7.9391e-01,  4.5966e-01, -1.4795e-01,  1.8337e+00, -3.8341e-01,\n",
      "          -7.4035e-01,  4.5308e-02, -6.1700e-01,  4.4745e-01, -8.9253e-01,\n",
      "          -1.6027e-01,  2.7352e-01, -7.9043e-01,  3.0036e-01, -1.7906e-01,\n",
      "           5.0054e-01, -3.6328e-01, -5.3378e-01, -6.0538e-01,  7.2216e-01,\n",
      "          -8.9520e-01, -4.6173e-01,  6.7817e-01,  1.3948e-01,  5.9521e-01,\n",
      "          -1.4073e-01,  4.5951e-01,  3.5103e-01, -1.8859e-01,  8.6729e-02,\n",
      "           5.9502e-01, -5.1658e-01, -6.7091e-03,  6.2310e-01,  1.7108e-01,\n",
      "          -4.1962e-01, -9.1596e-01,  4.1627e-01,  4.1864e-01, -1.9203e-01,\n",
      "           1.8489e-01, -2.2318e-01, -8.9769e-02, -1.0215e-01, -1.0433e-01,\n",
      "           1.5197e+00,  1.0473e+00, -6.1474e-01,  3.4961e-01,  9.4005e-02,\n",
      "          -4.3970e-01, -1.1027e+00,  7.9073e-01, -1.2182e-01,  7.8860e-01,\n",
      "           5.3579e-01, -4.7811e-01,  3.0030e-01,  5.8964e-01,  9.2987e-01,\n",
      "           3.9000e-01,  3.8941e-01,  1.8090e+00, -8.3105e-01, -1.4865e-01,\n",
      "          -2.4410e-01,  7.6533e-01,  8.0526e-02, -6.3911e-02, -9.2545e-02,\n",
      "           2.3288e-01, -3.9494e-01,  2.3344e-01, -1.2327e-01, -8.4012e-01,\n",
      "          -4.6843e-01,  4.7678e-01,  4.4935e-01,  9.8404e-02,  1.0942e+00,\n",
      "           5.0767e-01, -1.3610e+00,  5.0221e-01, -1.8202e-02, -1.0330e+00,\n",
      "          -5.4845e-01, -4.5033e-01, -2.5703e-01,  1.2963e+00, -2.8793e-01,\n",
      "           8.0815e-01,  2.5578e-01, -1.6829e-01, -4.3668e-01,  2.5367e-01,\n",
      "           4.8318e-01,  7.8105e-01,  7.5111e-01, -3.8102e-02,  7.3789e-01,\n",
      "          -6.3579e-02,  2.2830e-01, -1.3734e+00,  1.4432e+00,  3.7884e-01,\n",
      "           1.2605e+00, -7.9300e-01, -4.9394e-01,  4.5928e-02, -4.1731e-01,\n",
      "          -5.3632e-01,  1.8962e-01,  7.8236e-01, -1.4263e-01, -1.6087e-01,\n",
      "          -6.4520e-01,  1.6183e-01,  9.9159e-01,  3.5814e-01,  8.5340e-02,\n",
      "           5.1746e-01,  5.4370e-01,  1.7540e-01, -4.9678e-01, -4.8738e-01,\n",
      "          -1.9964e-01,  3.0989e-01, -7.6908e-01,  1.3707e-01,  7.1851e-01,\n",
      "          -1.1753e-01, -5.7134e-01, -3.3558e-02,  8.9272e-02, -1.9786e+00,\n",
      "          -1.2366e+00,  1.8289e+00, -2.0429e-01, -6.2334e-01,  3.9302e-01,\n",
      "           8.8277e-02,  3.1667e-01,  1.8783e-01, -2.9299e-01, -6.8184e-01,\n",
      "           6.7930e-01, -1.3405e+00,  4.7029e-01,  6.9198e-01,  1.0172e+00,\n",
      "          -7.0821e-01, -7.0272e-02, -8.8511e-01, -9.8676e-01, -1.5928e-01,\n",
      "          -5.9386e-01,  1.4762e-01,  5.4223e-01,  3.2550e-01, -4.1186e-02,\n",
      "          -1.9328e+00,  6.2252e-01, -7.2059e-01, -1.1274e+00, -2.5162e-01,\n",
      "          -1.4447e-01,  5.1927e-01, -6.1260e-01, -2.6738e-01,  1.8610e-01,\n",
      "          -2.6209e-01,  2.9987e-01,  9.0977e-02, -6.5315e-01, -6.4426e-01,\n",
      "           2.5911e-01, -6.1148e-02,  1.5774e+00, -7.8006e-01,  8.2914e-02,\n",
      "          -8.1570e-01, -6.0834e-01,  4.6815e-01,  2.9179e-01,  1.8014e-02,\n",
      "           3.5624e-01,  6.3908e-01,  9.7017e-01,  4.0858e-01, -1.2652e-01,\n",
      "           2.4386e-01,  3.3497e-01, -6.4510e-01,  2.5660e-01, -9.0793e-01,\n",
      "          -3.7920e-02,  1.1832e+00, -7.2423e-01,  1.8742e-01, -4.7096e-01,\n",
      "          -7.9281e-02,  7.6297e-01, -8.8909e-02, -4.2233e-01, -5.1579e-01,\n",
      "           6.8054e-01, -3.5112e-01, -1.7919e-01, -7.6789e-01,  1.4819e-01,\n",
      "           1.8024e-01,  1.2745e-01,  3.1256e-01, -2.6920e-01,  5.8286e-01,\n",
      "          -1.7616e+00,  1.4017e+00, -1.8952e-01, -4.7699e-01,  4.5128e-01,\n",
      "           2.7610e-01,  1.5782e-01, -3.8245e-01, -9.6448e-01, -7.2726e-01,\n",
      "           1.2794e-01,  4.7616e-01,  1.6262e-02, -5.6278e-01,  2.5688e-01,\n",
      "          -1.2845e+00, -2.7129e-01,  1.5806e+00, -2.5493e-01,  1.6977e-01,\n",
      "          -8.1133e-01, -8.1166e-01, -9.2160e-02,  7.5189e-01, -6.4404e-01,\n",
      "           1.3762e+00,  8.1821e-01,  4.4825e-02, -5.1846e-01,  4.4104e-01,\n",
      "           3.5159e-01,  6.1448e-01, -5.8847e-01, -5.4020e-01, -4.4213e-01,\n",
      "          -1.0324e+00, -1.9077e-01, -1.0375e+00,  8.2275e-01,  7.9395e-01,\n",
      "           5.7307e-01,  1.2196e+00, -2.4469e-01, -8.5815e-01, -9.4494e-01,\n",
      "          -1.3893e-01,  9.9153e-01,  4.0352e-01,  1.4285e-01,  6.6915e-01,\n",
      "           5.4766e-01, -4.4221e-01,  8.8268e-03, -4.1384e-01,  8.5077e-01,\n",
      "          -6.0541e-01,  1.2399e+00,  3.7287e-01, -4.8760e-01, -5.5724e-01,\n",
      "          -5.8970e-01, -1.1389e+00,  2.4673e+00,  9.8267e-01, -4.1463e-01,\n",
      "          -2.0950e-01,  6.2278e-02,  9.4794e-01, -1.5639e+00, -4.8146e-01,\n",
      "           5.6071e-01,  1.2044e+00,  1.2341e-01, -1.3694e-01, -5.6684e-01,\n",
      "          -3.6064e-01,  1.9630e-01, -2.9551e-01, -4.6581e-02,  5.2151e-01,\n",
      "          -3.9680e-01,  2.0343e-01,  7.7355e-01,  1.0102e-01, -5.6211e-01,\n",
      "          -6.9834e-01,  1.8654e-01,  6.6850e-01,  9.1453e-01,  2.0465e-01,\n",
      "          -5.3059e-01, -1.5448e-01,  4.0088e-01, -1.4755e+00,  2.6969e-02,\n",
      "          -2.3541e-01, -3.9453e-01,  6.2440e-01, -4.9355e-01, -3.8930e-01,\n",
      "          -2.3682e+00, -2.7228e-01,  5.2131e-01, -9.0522e-01,  7.2316e-01,\n",
      "          -7.4994e-01,  3.4919e-01,  6.3019e-01,  1.0868e-01,  8.4453e-02,\n",
      "           7.7599e-01, -1.9995e-01, -2.6032e-01,  8.8897e-02, -1.8674e-01,\n",
      "          -1.0914e-01,  6.1363e-01, -6.2204e-01,  6.2325e-01,  9.0556e-01,\n",
      "           5.1914e-01, -3.1240e-01, -1.3531e+00, -4.7157e-01, -4.4593e-01,\n",
      "          -7.9053e-02, -1.2200e+00,  1.3653e+00, -1.4812e-02, -3.0464e-02,\n",
      "          -6.9923e-01, -8.8633e-02, -3.9929e-01, -4.7419e-01,  2.8320e-01,\n",
      "           2.9362e-01, -3.8695e-02,  2.9797e-01, -5.7797e-02,  1.2704e+00,\n",
      "           3.3864e-01,  1.1173e-02,  9.4887e-01, -3.4893e-01, -2.8204e-01,\n",
      "          -6.4025e-01, -3.0037e-01, -8.6502e-01,  1.4441e+00,  4.7954e-01,\n",
      "          -1.4750e-01,  1.1188e-01,  7.2428e-01, -8.8116e-01, -5.3301e-01,\n",
      "           1.0764e+00,  1.3428e+00, -1.3011e-01, -1.7251e+00,  6.9558e-01,\n",
      "          -2.2014e-01, -1.1770e+00, -6.7139e-01,  1.2645e-02,  2.8829e-01,\n",
      "           1.2889e+00,  3.8172e-01,  3.0125e-01,  7.6155e-01,  4.6883e-01,\n",
      "           2.1765e-01,  8.2058e-01, -4.5185e-01,  5.1410e-01, -8.4310e-01,\n",
      "          -2.4684e-01, -3.8971e-01,  2.4382e-01, -8.5077e-01,  6.3445e-01,\n",
      "          -2.7886e-02, -3.9364e-01, -4.8225e-01, -4.4560e-01, -7.3249e-01,\n",
      "          -1.2222e+00, -4.4293e-02, -9.1889e-01,  3.6089e-01,  7.7820e-02,\n",
      "          -2.1086e-01, -4.5370e-01,  2.1817e-01, -2.9019e-01,  3.3228e-01,\n",
      "          -4.8083e-01, -2.3144e-01, -8.2706e-01, -7.7332e-01,  7.3194e-01,\n",
      "          -1.1053e+00, -1.5615e-01, -4.2222e-01, -3.2293e-01, -3.6955e-01,\n",
      "          -7.5790e-01, -7.2002e-01, -6.9829e-02,  8.0158e-01,  1.0233e+00,\n",
      "          -1.1142e-01,  4.1658e-01, -2.9659e-01, -1.1938e+00,  5.3761e-01,\n",
      "           2.9033e-01, -5.3159e-01, -3.2113e-01,  5.2059e-01,  8.6480e-01,\n",
      "          -8.6582e-02,  1.5092e+00,  2.8671e-01,  9.1929e-01, -1.7411e+00,\n",
      "           1.3544e+00,  1.8872e-01, -2.2183e-01,  4.9215e-01, -1.8581e-01,\n",
      "          -3.5198e-01, -3.8334e-01,  4.7430e-01,  1.4688e+00, -7.7874e-02,\n",
      "          -2.6202e-01, -1.4267e+00,  4.1563e-01,  6.4811e-01,  5.1883e-01,\n",
      "           9.2164e-02,  1.2707e-01, -9.9709e-02, -2.4284e-02,  2.3134e-01,\n",
      "          -8.4955e-01,  1.2553e-01, -1.3116e+00, -6.1481e-02, -6.8934e-01,\n",
      "           2.9938e-01, -7.5183e-02,  5.8571e-01, -2.1179e-01,  8.6865e-01,\n",
      "          -2.5892e-01,  1.5894e-01, -1.8817e-01,  4.5863e-01, -8.8975e-01,\n",
      "           1.9523e-01,  1.1142e+00,  2.9681e-01, -7.1801e-01,  1.0366e-01,\n",
      "          -4.4208e-01,  1.1144e+00, -7.9102e-01,  2.4641e-01, -4.3653e-03,\n",
      "           8.1098e-01,  3.0456e-01,  7.2766e-01, -1.1487e+00, -2.2771e-01,\n",
      "          -1.2286e-01, -8.9471e-01,  2.0915e-01,  6.1714e-01, -4.0839e-01,\n",
      "           6.2357e-01, -7.2469e-01, -1.2461e-01, -5.9653e-01, -1.3363e-01,\n",
      "          -1.0297e+00, -3.3428e-01, -2.0009e+00, -1.0085e+00,  3.4793e-01,\n",
      "           1.3037e+00,  6.1435e-01, -1.3728e-01, -9.2721e-01,  1.3277e+00,\n",
      "          -1.2765e-01,  1.7056e-01, -1.5549e+00,  7.6243e-01, -1.3682e+00,\n",
      "          -2.8027e-01,  1.8205e-01, -3.0846e-01,  9.8037e-02, -1.7203e-01,\n",
      "          -3.8473e-02, -3.4043e-01, -5.7426e-01,  8.2134e-01,  1.9970e-01,\n",
      "          -2.2257e-01, -1.4593e+00, -8.5484e-01, -4.2659e-01,  3.0970e-01,\n",
      "          -8.1092e-01, -7.2659e-01,  4.8128e-01, -8.0118e-01,  6.2954e-02,\n",
      "           6.0307e-01, -2.2705e-01, -1.0162e-01,  5.2791e-01,  4.1681e-02,\n",
      "           1.0146e+00,  5.0661e-01, -7.9144e-01,  2.3499e-01,  1.1470e+00,\n",
      "          -9.4177e-02, -1.0832e-01,  1.0355e+00, -8.2482e-01,  1.8090e-01,\n",
      "           3.5564e-01, -1.5052e+00,  7.5639e-01, -5.7389e-02, -4.6300e-01,\n",
      "          -5.9997e-01, -2.9622e-01, -3.8453e-01,  1.2539e-01, -4.1399e-01,\n",
      "          -2.1615e-01,  8.8209e-01, -2.7120e-01,  2.6205e-01,  2.1148e-01,\n",
      "          -2.4353e-01,  1.7761e-01,  2.0742e-01, -9.7357e-01,  1.3720e+00,\n",
      "          -8.5043e-02,  3.6967e-01, -9.7664e-01,  6.6233e-01,  3.4045e-01,\n",
      "           8.5836e-02, -1.0231e+00,  8.3651e-02, -9.5168e-02, -9.2769e-01,\n",
      "          -6.2186e-01,  1.4771e+00, -3.3386e-01, -2.9998e-03, -7.7928e-01,\n",
      "          -1.0993e+00, -3.6897e-01, -9.7183e-01,  5.4981e-02,  3.5147e-01,\n",
      "           1.7741e-01,  5.7180e-01,  1.4734e-01, -5.4747e-01,  4.3374e-01,\n",
      "           2.5915e-01, -3.7387e-01,  7.0060e-01,  7.3154e-01,  6.7239e-01,\n",
      "          -1.5450e-01,  3.7154e-01, -1.0753e+00,  4.8152e-01, -3.9791e-02,\n",
      "          -3.6171e-01, -2.1699e-01, -3.8296e-01,  1.5133e-02,  1.6382e+00,\n",
      "           6.1088e-01,  1.2371e-01,  6.6822e-01, -1.8889e-01,  3.1655e-01,\n",
      "          -4.9612e-01, -1.4523e+00,  5.2087e-01,  1.6620e-01, -4.0297e-01,\n",
      "           8.1326e-01,  3.9502e-03,  1.0386e+00,  1.6577e-01, -7.2786e-02,\n",
      "           3.1820e-01,  6.3716e-01,  3.4980e-02,  2.9033e-02,  1.3775e-01,\n",
      "          -1.3407e+00,  8.8676e-01,  2.1447e-01,  2.6299e-01, -9.5428e-01,\n",
      "           6.3152e-01, -1.0148e+00, -4.0980e-01, -1.2012e+00, -4.3062e-01,\n",
      "          -1.1700e-01,  7.8695e-02, -2.4081e-01,  1.3977e-01,  3.6212e-01,\n",
      "          -1.4673e-01,  1.3407e+00,  3.3653e-02, -1.0049e-01,  2.9391e-01,\n",
      "           3.1132e-01, -4.7039e-02,  4.1799e-01]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0]]]), tensor([[[-1.0371e+00,  3.9515e-01, -6.5867e-01, -7.2396e-02,  1.6084e+00,\n",
      "          -5.9844e-01, -7.1354e-01, -2.3577e-02,  7.6560e-01,  7.0373e-01,\n",
      "          -5.9151e-01,  5.9707e-01,  3.3662e-01,  2.8603e-01,  2.6011e-01,\n",
      "          -1.7486e-01, -5.4137e-01, -4.3599e-02,  8.5187e-01,  1.3756e+00,\n",
      "           1.8264e+00,  4.1690e-01, -4.3771e-01, -1.1941e-01, -6.6531e-01,\n",
      "          -4.7859e-02, -2.0685e-01, -1.0543e+00, -6.8715e-01,  1.4992e-01,\n",
      "           2.4614e-01,  3.4564e-01, -3.0112e-01, -4.2113e-02, -2.6306e-02,\n",
      "           8.2188e-01,  1.9872e-01,  7.0809e-02,  5.1735e-01,  1.7551e-01,\n",
      "           4.2995e-01,  5.8777e-01,  9.4538e-01,  2.2546e-01, -5.3248e-01,\n",
      "          -1.1268e+00,  3.6727e-01, -8.3205e-01, -1.7676e-01, -1.7672e-01,\n",
      "           2.2317e-03, -2.4613e-01,  7.3480e-01, -5.2762e-01,  3.1814e-01,\n",
      "           4.5334e-01,  1.5647e-02, -3.5377e-01,  4.6430e-01,  4.4255e-01,\n",
      "          -4.8431e-01, -1.2384e-01, -5.5225e-01, -3.6179e-01, -3.3160e-01,\n",
      "          -4.0192e-01,  2.2938e-01, -8.0931e-01, -5.6261e-01,  5.2370e-01,\n",
      "           5.6121e-01,  1.2388e-02, -9.8176e-01, -3.5730e-01, -9.0447e-01,\n",
      "           1.9532e+00,  8.7846e-01,  4.6020e-01,  1.0252e+00, -1.8131e-01,\n",
      "           1.1017e+00,  4.4659e-01,  3.7009e-01,  8.7765e-01, -9.4957e-01,\n",
      "          -5.6099e-01,  1.5106e-01, -1.3312e+00,  6.5580e-01, -3.7384e-01,\n",
      "           1.1190e+00, -4.0333e-01,  7.7124e-01,  8.2607e-02, -8.8927e-02,\n",
      "           5.7196e-01, -3.1273e-01, -2.9085e-01,  1.8818e-01, -2.6134e-01,\n",
      "          -8.0077e-01, -3.3680e-01,  4.9330e-01, -5.0678e-01,  8.1256e-01,\n",
      "          -3.3295e-01, -5.2752e-01, -1.0664e+00, -5.5914e-01, -1.2874e-01,\n",
      "          -4.1192e-01,  2.5329e-01, -4.9682e-01, -5.6379e-02,  6.1430e-01,\n",
      "          -7.9391e-01,  4.5966e-01, -1.4795e-01,  1.8337e+00, -3.8341e-01,\n",
      "          -7.4035e-01,  4.5308e-02, -6.1700e-01,  4.4745e-01, -8.9253e-01,\n",
      "          -1.6027e-01,  2.7352e-01, -7.9043e-01,  3.0036e-01, -1.7906e-01,\n",
      "           5.0054e-01, -3.6328e-01, -5.3378e-01, -6.0538e-01,  7.2216e-01,\n",
      "          -8.9520e-01, -4.6173e-01,  6.7817e-01,  1.3948e-01,  5.9521e-01,\n",
      "          -1.4073e-01,  4.5951e-01,  3.5103e-01, -1.8859e-01,  8.6729e-02,\n",
      "           5.9502e-01, -5.1658e-01, -6.7091e-03,  6.2310e-01,  1.7108e-01,\n",
      "          -4.1962e-01, -9.1596e-01,  4.1627e-01,  4.1864e-01, -1.9203e-01,\n",
      "           1.8489e-01, -2.2318e-01, -8.9769e-02, -1.0215e-01, -1.0433e-01,\n",
      "           1.5197e+00,  1.0473e+00, -6.1474e-01,  3.4961e-01,  9.4005e-02,\n",
      "          -4.3970e-01, -1.1027e+00,  7.9073e-01, -1.2182e-01,  7.8860e-01,\n",
      "           5.3579e-01, -4.7811e-01,  3.0030e-01,  5.8964e-01,  9.2987e-01,\n",
      "           3.9000e-01,  3.8941e-01,  1.8090e+00, -8.3105e-01, -1.4865e-01,\n",
      "          -2.4410e-01,  7.6533e-01,  8.0526e-02, -6.3911e-02, -9.2545e-02,\n",
      "           2.3288e-01, -3.9494e-01,  2.3344e-01, -1.2327e-01, -8.4012e-01,\n",
      "          -4.6843e-01,  4.7678e-01,  4.4935e-01,  9.8404e-02,  1.0942e+00,\n",
      "           5.0767e-01, -1.3610e+00,  5.0221e-01, -1.8202e-02, -1.0330e+00,\n",
      "          -5.4845e-01, -4.5033e-01, -2.5703e-01,  1.2963e+00, -2.8793e-01,\n",
      "           8.0815e-01,  2.5578e-01, -1.6829e-01, -4.3668e-01,  2.5367e-01,\n",
      "           4.8318e-01,  7.8105e-01,  7.5111e-01, -3.8102e-02,  7.3789e-01,\n",
      "          -6.3579e-02,  2.2830e-01, -1.3734e+00,  1.4432e+00,  3.7884e-01,\n",
      "           1.2605e+00, -7.9300e-01, -4.9394e-01,  4.5928e-02, -4.1731e-01,\n",
      "          -5.3632e-01,  1.8962e-01,  7.8236e-01, -1.4263e-01, -1.6087e-01,\n",
      "          -6.4520e-01,  1.6183e-01,  9.9159e-01,  3.5814e-01,  8.5340e-02,\n",
      "           5.1746e-01,  5.4370e-01,  1.7540e-01, -4.9678e-01, -4.8738e-01,\n",
      "          -1.9964e-01,  3.0989e-01, -7.6908e-01,  1.3707e-01,  7.1851e-01,\n",
      "          -1.1753e-01, -5.7134e-01, -3.3558e-02,  8.9272e-02, -1.9786e+00,\n",
      "          -1.2366e+00,  1.8289e+00, -2.0429e-01, -6.2334e-01,  3.9302e-01,\n",
      "           8.8277e-02,  3.1667e-01,  1.8783e-01, -2.9299e-01, -6.8184e-01,\n",
      "           6.7930e-01, -1.3405e+00,  4.7029e-01,  6.9198e-01,  1.0172e+00,\n",
      "          -7.0821e-01, -7.0272e-02, -8.8511e-01, -9.8676e-01, -1.5928e-01,\n",
      "          -5.9386e-01,  1.4762e-01,  5.4223e-01,  3.2550e-01, -4.1186e-02,\n",
      "          -1.9328e+00,  6.2252e-01, -7.2059e-01, -1.1274e+00, -2.5162e-01,\n",
      "          -1.4447e-01,  5.1927e-01, -6.1260e-01, -2.6738e-01,  1.8610e-01,\n",
      "          -2.6209e-01,  2.9987e-01,  9.0977e-02, -6.5315e-01, -6.4426e-01,\n",
      "           2.5911e-01, -6.1148e-02,  1.5774e+00, -7.8006e-01,  8.2914e-02,\n",
      "          -8.1570e-01, -6.0834e-01,  4.6815e-01,  2.9179e-01,  1.8014e-02,\n",
      "           3.5624e-01,  6.3908e-01,  9.7017e-01,  4.0858e-01, -1.2652e-01,\n",
      "           2.4386e-01,  3.3497e-01, -6.4510e-01,  2.5660e-01, -9.0793e-01,\n",
      "          -3.7920e-02,  1.1832e+00, -7.2423e-01,  1.8742e-01, -4.7096e-01,\n",
      "          -7.9281e-02,  7.6297e-01, -8.8909e-02, -4.2233e-01, -5.1579e-01,\n",
      "           6.8054e-01, -3.5112e-01, -1.7919e-01, -7.6789e-01,  1.4819e-01,\n",
      "           1.8024e-01,  1.2745e-01,  3.1256e-01, -2.6920e-01,  5.8286e-01,\n",
      "          -1.7616e+00,  1.4017e+00, -1.8952e-01, -4.7699e-01,  4.5128e-01,\n",
      "           2.7610e-01,  1.5782e-01, -3.8245e-01, -9.6448e-01, -7.2726e-01,\n",
      "           1.2794e-01,  4.7616e-01,  1.6262e-02, -5.6278e-01,  2.5688e-01,\n",
      "          -1.2845e+00, -2.7129e-01,  1.5806e+00, -2.5493e-01,  1.6977e-01,\n",
      "          -8.1133e-01, -8.1166e-01, -9.2160e-02,  7.5189e-01, -6.4404e-01,\n",
      "           1.3762e+00,  8.1821e-01,  4.4825e-02, -5.1846e-01,  4.4104e-01,\n",
      "           3.5159e-01,  6.1448e-01, -5.8847e-01, -5.4020e-01, -4.4213e-01,\n",
      "          -1.0324e+00, -1.9077e-01, -1.0375e+00,  8.2275e-01,  7.9395e-01,\n",
      "           5.7307e-01,  1.2196e+00, -2.4469e-01, -8.5815e-01, -9.4494e-01,\n",
      "          -1.3893e-01,  9.9153e-01,  4.0352e-01,  1.4285e-01,  6.6915e-01,\n",
      "           5.4766e-01, -4.4221e-01,  8.8268e-03, -4.1384e-01,  8.5077e-01,\n",
      "          -6.0541e-01,  1.2399e+00,  3.7287e-01, -4.8760e-01, -5.5724e-01,\n",
      "          -5.8970e-01, -1.1389e+00,  2.4673e+00,  9.8267e-01, -4.1463e-01,\n",
      "          -2.0950e-01,  6.2278e-02,  9.4794e-01, -1.5639e+00, -4.8146e-01,\n",
      "           5.6071e-01,  1.2044e+00,  1.2341e-01, -1.3694e-01, -5.6684e-01,\n",
      "          -3.6064e-01,  1.9630e-01, -2.9551e-01, -4.6581e-02,  5.2151e-01,\n",
      "          -3.9680e-01,  2.0343e-01,  7.7355e-01,  1.0102e-01, -5.6211e-01,\n",
      "          -6.9834e-01,  1.8654e-01,  6.6850e-01,  9.1453e-01,  2.0465e-01,\n",
      "          -5.3059e-01, -1.5448e-01,  4.0088e-01, -1.4755e+00,  2.6969e-02,\n",
      "          -2.3541e-01, -3.9453e-01,  6.2440e-01, -4.9355e-01, -3.8930e-01,\n",
      "          -2.3682e+00, -2.7228e-01,  5.2131e-01, -9.0522e-01,  7.2316e-01,\n",
      "          -7.4994e-01,  3.4919e-01,  6.3019e-01,  1.0868e-01,  8.4453e-02,\n",
      "           7.7599e-01, -1.9995e-01, -2.6032e-01,  8.8897e-02, -1.8674e-01,\n",
      "          -1.0914e-01,  6.1363e-01, -6.2204e-01,  6.2325e-01,  9.0556e-01,\n",
      "           5.1914e-01, -3.1240e-01, -1.3531e+00, -4.7157e-01, -4.4593e-01,\n",
      "          -7.9053e-02, -1.2200e+00,  1.3653e+00, -1.4812e-02, -3.0464e-02,\n",
      "          -6.9923e-01, -8.8633e-02, -3.9929e-01, -4.7419e-01,  2.8320e-01,\n",
      "           2.9362e-01, -3.8695e-02,  2.9797e-01, -5.7797e-02,  1.2704e+00,\n",
      "           3.3864e-01,  1.1173e-02,  9.4887e-01, -3.4893e-01, -2.8204e-01,\n",
      "          -6.4025e-01, -3.0037e-01, -8.6502e-01,  1.4441e+00,  4.7954e-01,\n",
      "          -1.4750e-01,  1.1188e-01,  7.2428e-01, -8.8116e-01, -5.3301e-01,\n",
      "           1.0764e+00,  1.3428e+00, -1.3011e-01, -1.7251e+00,  6.9558e-01,\n",
      "          -2.2014e-01, -1.1770e+00, -6.7139e-01,  1.2645e-02,  2.8829e-01,\n",
      "           1.2889e+00,  3.8172e-01,  3.0125e-01,  7.6155e-01,  4.6883e-01,\n",
      "           2.1765e-01,  8.2058e-01, -4.5185e-01,  5.1410e-01, -8.4310e-01,\n",
      "          -2.4684e-01, -3.8971e-01,  2.4382e-01, -8.5077e-01,  6.3445e-01,\n",
      "          -2.7886e-02, -3.9364e-01, -4.8225e-01, -4.4560e-01, -7.3249e-01,\n",
      "          -1.2222e+00, -4.4293e-02, -9.1889e-01,  3.6089e-01,  7.7820e-02,\n",
      "          -2.1086e-01, -4.5370e-01,  2.1817e-01, -2.9019e-01,  3.3228e-01,\n",
      "          -4.8083e-01, -2.3144e-01, -8.2706e-01, -7.7332e-01,  7.3194e-01,\n",
      "          -1.1053e+00, -1.5615e-01, -4.2222e-01, -3.2293e-01, -3.6955e-01,\n",
      "          -7.5790e-01, -7.2002e-01, -6.9829e-02,  8.0158e-01,  1.0233e+00,\n",
      "          -1.1142e-01,  4.1658e-01, -2.9659e-01, -1.1938e+00,  5.3761e-01,\n",
      "           2.9033e-01, -5.3159e-01, -3.2113e-01,  5.2059e-01,  8.6480e-01,\n",
      "          -8.6582e-02,  1.5092e+00,  2.8671e-01,  9.1929e-01, -1.7411e+00,\n",
      "           1.3544e+00,  1.8872e-01, -2.2183e-01,  4.9215e-01, -1.8581e-01,\n",
      "          -3.5198e-01, -3.8334e-01,  4.7430e-01,  1.4688e+00, -7.7874e-02,\n",
      "          -2.6202e-01, -1.4267e+00,  4.1563e-01,  6.4811e-01,  5.1883e-01,\n",
      "           9.2164e-02,  1.2707e-01, -9.9709e-02, -2.4284e-02,  2.3134e-01,\n",
      "          -8.4955e-01,  1.2553e-01, -1.3116e+00, -6.1481e-02, -6.8934e-01,\n",
      "           2.9938e-01, -7.5183e-02,  5.8571e-01, -2.1179e-01,  8.6865e-01,\n",
      "          -2.5892e-01,  1.5894e-01, -1.8817e-01,  4.5863e-01, -8.8975e-01,\n",
      "           1.9523e-01,  1.1142e+00,  2.9681e-01, -7.1801e-01,  1.0366e-01,\n",
      "          -4.4208e-01,  1.1144e+00, -7.9102e-01,  2.4641e-01, -4.3653e-03,\n",
      "           8.1098e-01,  3.0456e-01,  7.2766e-01, -1.1487e+00, -2.2771e-01,\n",
      "          -1.2286e-01, -8.9471e-01,  2.0915e-01,  6.1714e-01, -4.0839e-01,\n",
      "           6.2357e-01, -7.2469e-01, -1.2461e-01, -5.9653e-01, -1.3363e-01,\n",
      "          -1.0297e+00, -3.3428e-01, -2.0009e+00, -1.0085e+00,  3.4793e-01,\n",
      "           1.3037e+00,  6.1435e-01, -1.3728e-01, -9.2721e-01,  1.3277e+00,\n",
      "          -1.2765e-01,  1.7056e-01, -1.5549e+00,  7.6243e-01, -1.3682e+00,\n",
      "          -2.8027e-01,  1.8205e-01, -3.0846e-01,  9.8037e-02, -1.7203e-01,\n",
      "          -3.8473e-02, -3.4043e-01, -5.7426e-01,  8.2134e-01,  1.9970e-01,\n",
      "          -2.2257e-01, -1.4593e+00, -8.5484e-01, -4.2659e-01,  3.0970e-01,\n",
      "          -8.1092e-01, -7.2659e-01,  4.8128e-01, -8.0118e-01,  6.2954e-02,\n",
      "           6.0307e-01, -2.2705e-01, -1.0162e-01,  5.2791e-01,  4.1681e-02,\n",
      "           1.0146e+00,  5.0661e-01, -7.9144e-01,  2.3499e-01,  1.1470e+00,\n",
      "          -9.4177e-02, -1.0832e-01,  1.0355e+00, -8.2482e-01,  1.8090e-01,\n",
      "           3.5564e-01, -1.5052e+00,  7.5639e-01, -5.7389e-02, -4.6300e-01,\n",
      "          -5.9997e-01, -2.9622e-01, -3.8453e-01,  1.2539e-01, -4.1399e-01,\n",
      "          -2.1615e-01,  8.8209e-01, -2.7120e-01,  2.6205e-01,  2.1148e-01,\n",
      "          -2.4353e-01,  1.7761e-01,  2.0742e-01, -9.7357e-01,  1.3720e+00,\n",
      "          -8.5043e-02,  3.6967e-01, -9.7664e-01,  6.6233e-01,  3.4045e-01,\n",
      "           8.5836e-02, -1.0231e+00,  8.3651e-02, -9.5168e-02, -9.2769e-01,\n",
      "          -6.2186e-01,  1.4771e+00, -3.3386e-01, -2.9998e-03, -7.7928e-01,\n",
      "          -1.0993e+00, -3.6897e-01, -9.7183e-01,  5.4981e-02,  3.5147e-01,\n",
      "           1.7741e-01,  5.7180e-01,  1.4734e-01, -5.4747e-01,  4.3374e-01,\n",
      "           2.5915e-01, -3.7387e-01,  7.0060e-01,  7.3154e-01,  6.7239e-01,\n",
      "          -1.5450e-01,  3.7154e-01, -1.0753e+00,  4.8152e-01, -3.9791e-02,\n",
      "          -3.6171e-01, -2.1699e-01, -3.8296e-01,  1.5133e-02,  1.6382e+00,\n",
      "           6.1088e-01,  1.2371e-01,  6.6822e-01, -1.8889e-01,  3.1655e-01,\n",
      "          -4.9612e-01, -1.4523e+00,  5.2087e-01,  1.6620e-01, -4.0297e-01,\n",
      "           8.1326e-01,  3.9502e-03,  1.0386e+00,  1.6577e-01, -7.2786e-02,\n",
      "           3.1820e-01,  6.3716e-01,  3.4980e-02,  2.9033e-02,  1.3775e-01,\n",
      "          -1.3407e+00,  8.8676e-01,  2.1447e-01,  2.6299e-01, -9.5428e-01,\n",
      "           6.3152e-01, -1.0148e+00, -4.0980e-01, -1.2012e+00, -4.3062e-01,\n",
      "          -1.1700e-01,  7.8695e-02, -2.4081e-01,  1.3977e-01,  3.6212e-01,\n",
      "          -1.4673e-01,  1.3407e+00,  3.3653e-02, -1.0049e-01,  2.9391e-01,\n",
      "           3.1132e-01, -4.7039e-02,  4.1799e-01]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0]]]))\n"
     ]
    }
   ],
   "source": [
    "input_ids = df['english_encoded'].values.tolist()\n",
    "target_ids = df['turkish_encoded'].values.tolist()\n",
    "attention_masks = df['attention_masks'].values.tolist()\n",
    "decoder_attention_masks = df['decoder_attention_masks'].values.tolist()\n",
    "\n",
    "#create custom dataset that takes in the input_ids, attention_masks, and target_ids\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, decoder_attention_masks, target_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.target_ids = target_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.decoder_attention_masks = decoder_attention_masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_ids[idx]\n",
    "        target_ids = self.target_ids[idx]\n",
    "        attention_masks = self.attention_masks[idx]\n",
    "        decoder_attention_masks = self.decoder_attention_masks[idx]\n",
    "        return input_ids, attention_masks, target_ids, decoder_attention_masks\n",
    "\n",
    "dataset = CustomDataset(input_ids, attention_masks, decoder_attention_masks, input_ids)\n",
    "print(dataset[0])\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "50257\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# decoder = gpt2_model.decoder\n",
    "print(gpt2_model.config.n_positions)\n",
    "print(gpt2_model.config.vocab_size)\n",
    "print(gpt2_model.config.n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m     31\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m train(model, dataloader, optimizer, device)\n",
      "Cell \u001b[0;32mIn[68], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 24\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     26\u001b[0m \u001b[39m# Print training information\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBatch Loss:\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:116\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m@_use_grad_for_differentiable\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, closure\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Performs a single optimization step.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m        closure (Callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m            and returns the loss.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cuda_graph_capture_health_check()\n\u001b[1;32m    118\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:230\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_graph_capture_health_check\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mhas_cuda \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 230\u001b[0m         capturing \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_current_stream_capturing()\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m capturing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(group[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups):\n\u001b[1;32m    233\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m    234\u001b[0m                                \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m    235\u001b[0m                                \u001b[39m\"\u001b[39m\u001b[39m but param_groups\u001b[39m\u001b[39m'\u001b[39m\u001b[39m capturable is False.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/graphs.py:25\u001b[0m, in \u001b[0;36mis_current_stream_capturing\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_current_stream_capturing\u001b[39m():\n\u001b[1;32m     20\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m    Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[39m    If a CUDA context does not exist on the current device, returns False without initializing the context.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     \u001b[39mreturn\u001b[39;00m _cuda_isCurrentStreamCapturing()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # print(batch)\n",
    "        # print(batch[0])\n",
    "        input_ids = batch[0].to(device)\n",
    "        target_ids = batch[1].to(device)\n",
    "        input_attention_mask = batch[2].to(device)\n",
    "        target_attention_mask = batch[3].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        input_embeddings = torch.randn(1, 768, 768)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs_embeds = input_embeddings, labels=target_ids)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print training information\n",
    "        print('Batch Loss:', loss.item())\n",
    "    \n",
    "model = gpt2_model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device(\"cpu\")\n",
    "train(model, dataloader, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
